import pandas as pd
import numpy as np
import itertools
import os
import time
from joblib import Parallel, delayed
from tqdm import tqdm

# Utility Function Imports (only import those needed)
from utilities import *

# Algorithm Imports
from Strategies.benchmarks import *
from Strategies.follow_the_loser import *
from Strategies.follow_the_winner import *
from Strategies.pattern_matching import *
from Strategies.meta_learning import *

# Make sure the following algorithms are imported or defined in your project:
# cwmr, follow_the_regularized_leader, pamr, anticor

# -------------------------------
# PART 1: Parameter Tuning Setup
# -------------------------------
price_relative_df = pd.read_csv("..\\Data\\Price Relative Vectors\\price_relative_vectors.csv", index_col=0)
b = initialize_portfolio(price_relative_df.shape[1])
price_relative_vectors_array = price_relative_df.values
dates = price_relative_df.index

# -------------------------------
# Evaluation Function for Fast Universalization Strategy
# -------------------------------
def evaluate_fast_universalization(b, price_relative_vectors, learning_rate, base_experts):
    """
    Runs the Fast Universalization strategy with the given hyperparameters and returns performance metrics.
    
    Parameters:
        b: Initial portfolio (numpy array).
        price_relative_vectors: numpy array of price relative vectors (shape: [T, N]).
        learning_rate: Hyperparameter for the learning rate.
        base_experts: List of functions representing the base experts.
        
    Returns:
        A dictionary containing performance metrics.
    """
    # Run the fast universalization strategy with the provided hyperparameters
    b_n = fast_universalization(b, price_relative_vectors, learning_rate=learning_rate, base_experts=base_experts)
    
    # Calculate performance metrics
    final_wealth = calculate_cumulative_wealth(b_n, price_relative_vectors)
    n_periods = len(price_relative_vectors)
    exp_growth = calculate_exponential_growth_rate(final_wealth, n_periods)
    cum_wealth = calculate_cumulative_wealth_over_time(b_n, price_relative_vectors)
    daily_returns = compute_periodic_returns(cum_wealth)
    sharpe = compute_sharpe_ratio(daily_returns)
    
    return {
        'algorithm': 'fast_universalization',
        'learning_rate': learning_rate,
        'base_experts': str([expert.__name__ for expert in base_experts]),
        'final_wealth': final_wealth,
        'exp_growth': exp_growth,
        'sharpe': sharpe
    }

# -------------------------------
# Wrapper Function for Joblib Evaluation
# -------------------------------
def evaluate_fast_uni_combo(param_dict, b, price_relative_vectors_array):
    """
    Wrapper that extracts hyperparameters from the combination dictionary and calls evaluate_fast_universalization.
    """
    try:
        learning_rate = param_dict['learning_rate']
        base_experts = param_dict['base_experts']
        result = evaluate_fast_universalization(b, price_relative_vectors_array, learning_rate, base_experts)
        # Ensure grid parameters are recorded
        result['learning_rate'] = learning_rate
        result['base_experts'] = str([expert.__name__ for expert in base_experts])
        return result
    except Exception as e:
        print(f"‚ùå ERROR: {param_dict} | Exception: {str(e)}")
        return {
            'learning_rate': param_dict.get('learning_rate'),
            'base_experts': str([expert.__name__ for expert in param_dict.get('base_experts', [])]),
            'final_wealth': None,
            'exp_growth': None,
            'sharpe': None,
            'error': str(e)
        }

# -------------------------------
# Generate Valid Parameter Combinations
# -------------------------------
def generate_valid_combinations(grid):
    """
    Generates a list of parameter dictionaries from the grid for fast universalization.
    """
    valid_combinations = []
    for lr in grid['learning_rate']:
        for experts in grid['base_experts']:
            valid_combinations.append({
                'learning_rate': lr,
                'base_experts': experts
            })
    return valid_combinations

# -------------------------------
# Build Comprehensive Base Experts List
# -------------------------------
# Consider the four algorithms: cwmr, follow_the_regularized_leader, pamr, anticor.
# Generate all combinations of size 2 or more.
base_algorithms = [cwmr, follow_the_regularized_leader, pamr, anticor]
all_base_expert_combinations = []
for r in range(2, len(base_algorithms) + 1):
    for combo in itertools.combinations(base_algorithms, r):
        all_base_expert_combinations.append(list(combo))

# -------------------------------
# Joblib Grid Search for Fast Universalization
# -------------------------------
def run_joblib_grid_fast_uni(grid, b, price_relative_vectors_array, n_jobs=4):
    param_dicts = generate_valid_combinations(grid)
    print(f"üîÅ Starting joblib tuning for fast universalization with {len(param_dicts)} combinations on {n_jobs} workers")
    
    results = Parallel(n_jobs=n_jobs)(
        delayed(evaluate_fast_uni_combo)(param_dict, b, price_relative_vectors_array)
        for param_dict in tqdm(param_dicts, desc="Tuning Progress")
    )
    
    results_df = pd.DataFrame(results)
    results_df.sort_values(by='sharpe', ascending=False, inplace=True)
    best_result = results_df.iloc[0].to_dict()
    return results_df, best_result

# -------------------------------
# Define the grid for Fast Universalization tuning
# -------------------------------
fast_uni_grid = {
    'learning_rate': [0.1, 0.2, 0.5, 0.8, 1.0],
    'base_experts': all_base_expert_combinations
}

# -------------------------------
# Run the Parallelized Tuning for Fast Universalization Strategy
# -------------------------------
if __name__ == '__main__':
    n_jobs = max(1, os.cpu_count() - 2)
    print(f"Detected {os.cpu_count()} logical cores. Using {n_jobs} workers for parallel tuning.")
    
    start = time.time()
    fast_uni_results_df, best_fast_uni = run_joblib_grid_fast_uni(fast_uni_grid, b, price_relative_vectors_array, n_jobs=n_jobs)
    
    # Save results for future reference
    fast_uni_results_df.to_csv("..\\Data\\Tuning Data\\fast_uni_tuning_results.csv", index=False)
    
    elapsed = time.time() - start
    print(f"\n‚úÖ Tuning complete in {elapsed/60:.2f} minutes.")
    print("üî• Best Fast Universalization configuration found:")
    print(best_fast_uni)
